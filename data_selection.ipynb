{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cc1bf0e-e80b-4993-b388-aa77fdc2ee06",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from geopy.geocoders import Nominatim\n",
    "from openai import OpenAI\n",
    "import requests\n",
    "import re\n",
    "from defines import coordinates_dict\n",
    "import glob\n",
    "import difflib\n",
    "import pickle\n",
    "from multiprocessing import Pool, cpu_count\n",
    "import unicodedata\n",
    "from ast import literal_eval\n",
    "from statistics import mode\n",
    "import os\n",
    "from pathlib import Path\n",
    "import shutil\n",
    "from odf import text, teletype\n",
    "from odf.opendocument import load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5543176-57bd-48f4-ae14-28555b668dbb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "directory_path  = \"./local_data/data/data_selection/**/\"  # Replace with the path to your folder\n",
    "all_csv_files_path  = glob.glob(f\"{directory_path}/*.csv\", recursive=True)\n",
    "all_odt_files_path = glob.glob(f\"{directory_path}/*.odt\", recursive=True)\n",
    "\n",
    "# Display the list of CSV files\n",
    "print(f\"CSV files in the folder:{len(all_csv_files_path)}\")\n",
    "for csv_file_path in all_csv_files_path: #sorted(all_csv_files_path, key=lambda x: Path(x).stem):\n",
    "    print(csv_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "993f7ddb-dcdc-4ce5-a0d4-02a3c6a7f339",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rename_columns(df, first_column='Unnamed: 0'):\n",
    "    return df.rename(columns={first_column:'scientificName'})\n",
    "\n",
    "def remove_extra_space(text):\n",
    "    if text !=text:\n",
    "        return text\n",
    "    text = re.sub(' +', ' ', text).strip()\n",
    "    return text\n",
    "\n",
    "# Clean and correct names\n",
    "def remove_number(text):\n",
    "    if text !=text:\n",
    "        return text\n",
    "    text = re.sub('^[0-9.\\*]*', '', text, count=1)\n",
    "    text = re.sub('^[aA-zZ]\\\\)', '', text, count=1)\n",
    "    return text\n",
    "\n",
    "def replace_commas(text):\n",
    "    if text !=text:\n",
    "        return text\n",
    "    text = re.sub('^, ,', ' ,, ', text)\n",
    "    text = re.sub('^，，', ',,', text)\n",
    "    text = re.sub('^,,', ' ,, ', text)\n",
    "    text = re.sub('^,', ' ,, ', text)\n",
    "    text = re.sub('^, , , ,', ' ,, ,, ', text)\n",
    "    return text\n",
    "\n",
    "def normalize_to_ascii(text):\n",
    "    try:\n",
    "        return unicodedata.normalize('NFKD', text).encode('ascii', 'ignore').decode('utf-8')\n",
    "    except:\n",
    "        return text\n",
    "        \n",
    "def remove_roman_numerals(string):\n",
    "    pattern = r'^((M{0,4})(CM|CD|D?C{0,3})(XC|XL|L?X{0,3})(IX|IV|V?I{0,3}))\\.'\n",
    "    result = re.sub(pattern, '', string)\n",
    "    return result.strip()\n",
    "\n",
    "def complete_species_name(scientificName_list, i):\n",
    "    prev = scientificName_list[i-1].split()[0]\n",
    "    # print('0', scientificName_list[i], '\\t', scientificName_list[i-1])\n",
    "    # print(prev)\n",
    "    scientificName_list[i] = scientificName_list[i].replace(',,', prev)\n",
    "    # print('1', scientificName_list[i], '\\t', scientificName_list[i-1], end='\\n\\n')\n",
    "    return scientificName_list[i]\n",
    "\n",
    "def get_close_scname_and_data_gbif_list(text):\n",
    "    # url = \"https://api.gbif.org/v1/species/search?q={}&origin=SOURCE&status=ACCEPTED&strict=true\".format(text)\n",
    "    url = \"https://api.gbif.org/v1/species/match?name={}&status=ACCEPTED&strict=false&verbose=true\".format(text)\n",
    "    payload = {}\n",
    "    # headers = {'Authorization': 'Basic YWtodnlhczA6VnlAJDEyMzQ='}\n",
    "    headers = {}\n",
    "    response = requests.request(\"GET\", url, headers=headers, data=payload)\n",
    "    # print(text)\n",
    "    try:\n",
    "        if response.status_code==200:\n",
    "            if 'scientificName' in response.json():\n",
    "                alt_list = []\n",
    "                alt_list.append((response.json()['scientificName'],\\\n",
    "                   response.json()['confidence'], response.json()['kingdom']))\n",
    "                return alt_list, len(alt_list)\n",
    "            else:\n",
    "                try:\n",
    "                    alt_list = []\n",
    "                    # print('len: ', response.json()['alternatives'])\n",
    "                    confidence = response.json()['alternatives'][0]['confidence']\n",
    "                    # print('len: ', response.json()['alternatives'])\n",
    "                    for i in range(len(response.json()['alternatives'])):\n",
    "                        if response.json()['alternatives'][i]['confidence']==confidence:\n",
    "                            alt_list.append((response.json()['alternatives'][i]['scientificName'],\\\n",
    "                                              response.json()['alternatives'][i]['confidence'],\\\n",
    "                                              response.json()['alternatives'][i]['kingdom']))\n",
    "                        else:\n",
    "                            break\n",
    "                    return alt_list, len(alt_list)      \n",
    "                except Exception as e:\n",
    "                    alt_list = []\n",
    "                    alt_list.append((None, None, None))\n",
    "                    return alt_list, None\n",
    "        else:\n",
    "            alt_list = []\n",
    "            alt_list.append((None, None, None))\n",
    "            return alt_list, None\n",
    "    except Exception as e: \n",
    "        # print('Except: ', e, response.text, end='\\n\\n\\n\\n')\n",
    "        alt_list = []\n",
    "        alt_list.append((None, None, None))\n",
    "        return alt_list, None\n",
    "\n",
    "def get_close_scname_and_data_from_dict_gbif_list(sc_name, close_match_sc_dict_gbif_list):\n",
    "    if sc_name in close_match_sc_dict_gbif_list:\n",
    "        return close_match_sc_dict_gbif_list[sc_name]\n",
    "    alt_list = []\n",
    "    alt_list.append((None, None, None))\n",
    "    return alt_list, None\n",
    "\n",
    "def candidate_selection(candidate_list_of_list):\n",
    "    candidate_list_of_list_new = []\n",
    "    for i, candidate_list in enumerate(candidate_list_of_list):\n",
    "        candidate = candidate_list_of_list[i]\n",
    "        if candidate_list:\n",
    "            if len(candidate_list)>1:\n",
    "                mode_kingdom = mode([k[2] for j in get_surrounding_index(candidate_list_of_list, i) for k in j])\n",
    "                candidate = [i for i in candidate_list if mode_kingdom in i]\n",
    "        candidate_list_of_list_new.append(candidate)\n",
    "    return candidate_list_of_list_new  \n",
    "\n",
    "def get_surrounding_index(lst, index):\n",
    "    if index==0 or index==1 or index==2:\n",
    "        return lst[0:5]\n",
    "    elif index==len(lst)-1 or index==len(lst)-2 or index==len(lst)-3:\n",
    "        return lst[len(lst)-5:len(lst)]\n",
    "    else:\n",
    "        return lst[index-2:index+3]\n",
    "\n",
    "def create_directory_from_file_path(file_path):\n",
    "    # Extract the directory path\n",
    "    directory_path = os.path.dirname(file_path)\n",
    "\n",
    "    # Check if the directory exists\n",
    "    if not os.path.exists(directory_path):\n",
    "        # Create the directory\n",
    "        os.makedirs(directory_path)\n",
    "    return directory_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffc05638-e620-418c-a14a-f6ac98f606ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting location description \n",
    "def odt_to_txt(input_file):\n",
    "    doc = load(input_file)\n",
    "    txt = ''\n",
    "    for paragraph in doc.getElementsByType(text.P):\n",
    "        txt += teletype.extractText(paragraph) + '\\n'\n",
    "    return txt\n",
    "\n",
    "def extract_numbers(text):\n",
    "    try: \n",
    "        if re.match('\\d+', text):\n",
    "            number = int(re.match('\\d+', text).group())\n",
    "            return number\n",
    "        elif re.match('[aA-zZ\\s+]+:', text):\n",
    "            number = re.match('[aA-zZ\\s+]+:', text).group().replace(':', '')\n",
    "            return number\n",
    "    except Exception as e:\n",
    "        print('Extract Number', e)\n",
    "        return None\n",
    "    return None\n",
    "\n",
    "def split_by_multiple_blank_lines(text):\n",
    "    sections = re.split('\\n\\n+', text)\n",
    "    return sections\n",
    "\n",
    "def odt_to_dict(input_file):\n",
    "    txt = odt_to_txt(input_file)\n",
    "    paras = split_by_multiple_blank_lines(txt)\n",
    "    num_para_dict = {extract_numbers(para):para for para in paras if extract_numbers(para)}\n",
    "    if None in num_para_dict:\n",
    "        num_para_dict.pop(None)\n",
    "    return num_para_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5939f997-9583-4b09-87a8-37dd29952a2a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "count_file = 0\n",
    "close_match_sc_dict_gbif_list =  dict()\n",
    "for csv_file_path in all_csv_files_path:\n",
    "    try:\n",
    "        file_id = int(csv_file_path.split(\"/\")[-1].split(\".\")[0])\n",
    "        df = pd.read_csv(filepath_or_buffer=csv_file_path, encoding='utf-8')\n",
    "        df = df.map(remove_extra_space, na_action='ignore')\n",
    "        first_column = df.columns[0]\n",
    "        df = rename_columns(df, first_column)\n",
    "        df['scientificName'] = df['scientificName'].apply(remove_number)\n",
    "        df = df.map(remove_extra_space, na_action='ignore')\n",
    "        df['scientificName'] = df['scientificName'].apply(replace_commas)\n",
    "        df['scientificName'] = df['scientificName'].apply(normalize_to_ascii)\n",
    "        df['scientificName'] = df['scientificName'].apply(remove_roman_numerals)\n",
    "        df = df.map(remove_extra_space, na_action='ignore')\n",
    "        scientificName_list = df['scientificName'].tolist()\n",
    "\n",
    "        # Complete scientificName\n",
    "        df['scientificName'] = [scientificName_list[0]]+ [complete_species_name(scientificName_list, i) for i, j in enumerate(scientificName_list) if i>0]\n",
    "        close_match_sc_dict_gbif_list.update({sc_name:get_close_scname_and_data_gbif_list(sc_name) for sc_name in df['scientificName'].unique().tolist()})\n",
    "\n",
    "        # Fuzzy Match Scientific Names\n",
    "        df[['scientificName_matchingScore_kingdom_CloseGbiflist', 'scientificName_matchingScore_kingdom_CloseGbiflistLength']]\\\n",
    "                    = pd.DataFrame(df['scientificName'].\\\n",
    "                                   apply(get_close_scname_and_data_from_dict_gbif_list, args=(close_match_sc_dict_gbif_list,)).\\\n",
    "                                   tolist(), index=df.index)\n",
    "    \n",
    "        df['scientificName_matchingScore_kingdom_CloseGbif_Candidate'] = [i[0] for i in candidate_selection(df['scientificName_matchingScore_kingdom_CloseGbiflist']\\\n",
    "                                                                                               .tolist())]\n",
    "        df[['scientificNameGbif', 'matchingScoreGbif', 'kingdomGbif']]= pd.DataFrame(df['scientificName_matchingScore_kingdom_CloseGbif_Candidate'].tolist(),index=df.index)\n",
    "\n",
    "\n",
    "\n",
    "        # save data\n",
    "        if df['matchingScoreGbif'].mean() >=60:\n",
    "            directory_path = os.path.dirname(csv_file_path)\n",
    "            csv_file_path = csv_file_path.replace('./local_data/data/data_selection/', './data/selected_data/')\n",
    "            # path of cleaned data\n",
    "            csv_file_path_dir = create_directory_from_file_path(csv_file_path)\n",
    "            \n",
    "            # src - dest\n",
    "            num_para_dict = odt_to_dict(os.path.join(directory_path, str(file_id) +'.odt'))\n",
    "            df_odt = pd.DataFrame(list(num_para_dict.items()), columns=['Index', 'Location Description'])\n",
    "            df_odt.to_csv(os.path.join(csv_file_path_dir, str(file_id) +'_odt.csv'), encoding='utf-8', index=False)\n",
    "            \n",
    "            df.to_csv(os.path.join(csv_file_path_dir, str(file_id) +'.csv'), encoding='utf-8', index=False)\n",
    "            count_file = count_file + 1 \n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing location '{csv_file_path}': {e}\")\n",
    "        print(f\"Error processing location '{first_column}': {df.columns}\", end='\\n\\n\\n\\n')\n",
    "\n",
    "print(\"Files with correct scientific names: \", count_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c8f1e94-2480-42a5-876a-88d53842d6a2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
