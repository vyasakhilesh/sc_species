{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3cc1bf0e-e80b-4993-b388-aa77fdc2ee06",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from geopy.geocoders import Nominatim\n",
    "from openai import OpenAI\n",
    "import requests\n",
    "import re\n",
    "from defines import coordinates_dict\n",
    "import glob\n",
    "import difflib\n",
    "import pickle\n",
    "from multiprocessing import Pool, cpu_count\n",
    "import unicodedata\n",
    "from ast import literal_eval\n",
    "from statistics import mode\n",
    "import os\n",
    "from pathlib import Path\n",
    "import shutil\n",
    "from odf import text, teletype\n",
    "from odf.opendocument import load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5543176-57bd-48f4-ae14-28555b668dbb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "directory_path  = \"./local_data/data/data_selection/**/\"  # Replace with the path to your folder\n",
    "all_csv_files_path  = glob.glob(f\"{directory_path}/*.csv\", recursive=True)\n",
    "all_odt_files_path = glob.glob(f\"{directory_path}/*.odt\", recursive=True)\n",
    "\n",
    "# Display the list of CSV files\n",
    "print(f\"CSV files in the folder:{len(all_csv_files_path)}\")\n",
    "for csv_file_path in all_csv_files_path: #sorted(all_csv_files_path, key=lambda x: Path(x).stem):\n",
    "    print(csv_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "993f7ddb-dcdc-4ce5-a0d4-02a3c6a7f339",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:16: SyntaxWarning: invalid escape sequence '\\*'\n",
      "<>:16: SyntaxWarning: invalid escape sequence '\\*'\n",
      "/tmp/ipykernel_7390/3690144397.py:16: SyntaxWarning: invalid escape sequence '\\*'\n",
      "  text = re.sub('^[0-9.\\*]*', '', text, count=1)\n"
     ]
    }
   ],
   "source": [
    "def rename_columns(df, first_column='Unnamed: 0'):\n",
    "    return df.rename(columns={first_column:'scientificName'})\n",
    "\n",
    "def remove_extra_space(text):\n",
    "    text = str(text)\n",
    "    if text !=text:\n",
    "        return text\n",
    "    text = re.sub(' +', ' ', text).strip()\n",
    "    return text\n",
    "\n",
    "# Clean and correct names\n",
    "def remove_number(text):\n",
    "    text = str(text)\n",
    "    if text !=text:\n",
    "        return text\n",
    "    text = re.sub('^[0-9.\\*]*', '', text, count=1)\n",
    "    text = re.sub('^[aA-zZ]\\\\)', '', text, count=1)\n",
    "    return text\n",
    "\n",
    "def replace_commas(text):\n",
    "    text = str(text)\n",
    "    if text !=text:\n",
    "        return text\n",
    "    text = re.sub('^, ,', ' ,, ', text)\n",
    "    text = re.sub('^，，', ',,', text)\n",
    "    text = re.sub('^,,', ' ,, ', text)\n",
    "    text = re.sub('^,', ' ,, ', text)\n",
    "    text = re.sub('^, , , ,', ' ,, ,, ', text)\n",
    "    return text\n",
    "\n",
    "def normalize_to_ascii(text):\n",
    "    try:\n",
    "        return unicodedata.normalize('NFKD', text).encode('ascii', 'ignore').decode('utf-8')\n",
    "    except:\n",
    "        return text\n",
    "        \n",
    "def remove_roman_numerals(text):\n",
    "    text = str(text)\n",
    "    pattern = r'^((M{0,4})(CM|CD|D?C{0,3})(XC|XL|L?X{0,3})(IX|IV|V?I{0,3}))\\.'\n",
    "    result = re.sub(pattern, '', text)\n",
    "    return result.strip()\n",
    "\n",
    "def complete_species_name(scientificName_list, i):\n",
    "    prev = scientificName_list[i-1].split()[0]\n",
    "    # print('0', scientificName_list[i], '\\t', scientificName_list[i-1])\n",
    "    # print(prev)\n",
    "    scientificName_list[i] = scientificName_list[i].replace(',,', prev)\n",
    "    # print('1', scientificName_list[i], '\\t', scientificName_list[i-1], end='\\n\\n')\n",
    "    return scientificName_list[i]\n",
    "\n",
    "def get_close_scname_and_data_gbif_list(text):\n",
    "    text = str(text)\n",
    "    # url = \"https://api.gbif.org/v1/species/search?q={}&origin=SOURCE&status=ACCEPTED&strict=true\".format(text)\n",
    "    url = \"https://api.gbif.org/v1/species/match?name={}&status=ACCEPTED&strict=false&verbose=true\".format(text)\n",
    "    payload = {}\n",
    "    # headers = {'Authorization': 'Basic YWtodnlhczA6VnlAJDEyMzQ='}\n",
    "    headers = {}\n",
    "    response = requests.request(\"GET\", url, headers=headers, data=payload)\n",
    "    # print(text)\n",
    "    try:\n",
    "        if response.status_code==200:\n",
    "            if 'scientificName' in response.json():\n",
    "                alt_list = []\n",
    "                alt_list.append((response.json()['scientificName'],\\\n",
    "                   response.json()['confidence'], response.json()['kingdom']))\n",
    "                return alt_list, len(alt_list)\n",
    "            else:\n",
    "                try:\n",
    "                    alt_list = []\n",
    "                    # print('len: ', response.json()['alternatives'])\n",
    "                    confidence = response.json()['alternatives'][0]['confidence']\n",
    "                    # print('len: ', response.json()['alternatives'])\n",
    "                    for i in range(len(response.json()['alternatives'])):\n",
    "                        if response.json()['alternatives'][i]['confidence']==confidence:\n",
    "                            alt_list.append((response.json()['alternatives'][i]['scientificName'],\\\n",
    "                                              response.json()['alternatives'][i]['confidence'],\\\n",
    "                                              response.json()['alternatives'][i]['kingdom']))\n",
    "                        else:\n",
    "                            break\n",
    "                    return alt_list, len(alt_list)      \n",
    "                except Exception as e:\n",
    "                    alt_list = []\n",
    "                    alt_list.append((None, None, None))\n",
    "                    return alt_list, None\n",
    "        else:\n",
    "            alt_list = []\n",
    "            alt_list.append((None, None, None))\n",
    "            return alt_list, None\n",
    "    except Exception as e: \n",
    "        # print('Except: ', e, response.text, end='\\n\\n\\n\\n')\n",
    "        alt_list = []\n",
    "        alt_list.append((None, None, None))\n",
    "        return alt_list, None\n",
    "\n",
    "def get_close_scname_and_data_from_dict_gbif_list(sc_name, close_match_sc_dict_gbif_list):\n",
    "    if sc_name in close_match_sc_dict_gbif_list:\n",
    "        return close_match_sc_dict_gbif_list[sc_name]\n",
    "    alt_list = []\n",
    "    alt_list.append((None, None, None))\n",
    "    return alt_list, None\n",
    "\n",
    "def candidate_selection(candidate_list_of_list):\n",
    "    candidate_list_of_list_new = []\n",
    "    for i, candidate_list in enumerate(candidate_list_of_list):\n",
    "        candidate = candidate_list_of_list[i]\n",
    "        if candidate_list:\n",
    "            if len(candidate_list)>1:\n",
    "                mode_kingdom = mode([k[2] for j in get_surrounding_index(candidate_list_of_list, i) for k in j])\n",
    "                candidate = [i for i in candidate_list if mode_kingdom in i]\n",
    "        candidate_list_of_list_new.append(candidate)\n",
    "    return candidate_list_of_list_new  \n",
    "\n",
    "def get_surrounding_index(lst, index):\n",
    "    if index==0 or index==1 or index==2:\n",
    "        return lst[0:5]\n",
    "    elif index==len(lst)-1 or index==len(lst)-2 or index==len(lst)-3:\n",
    "        return lst[len(lst)-5:len(lst)]\n",
    "    else:\n",
    "        return lst[index-2:index+3]\n",
    "\n",
    "def create_directory_from_file_path(file_path):\n",
    "    # Extract the directory path\n",
    "    directory_path = os.path.dirname(file_path)\n",
    "\n",
    "    # Check if the directory exists\n",
    "    if not os.path.exists(directory_path):\n",
    "        # Create the directory\n",
    "        os.makedirs(directory_path)\n",
    "    return directory_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffc05638-e620-418c-a14a-f6ac98f606ce",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Extracting location description \n",
    "def odt_to_txt(input_file):\n",
    "    doc = load(input_file)\n",
    "    txt = ''\n",
    "    for paragraph in doc.getElementsByType(text.P):\n",
    "        txt += teletype.extractText(paragraph) + '\\n'\n",
    "    return txt\n",
    "\n",
    "def extract_numbers(text):\n",
    "    try: \n",
    "        if re.match('\\d+', text):\n",
    "            number = int(re.match('\\d+', text).group())\n",
    "            return int(number)\n",
    "        elif re.match('[aA-zZäöüÄÖÜß\\s+]+:', text):\n",
    "            number = re.match('[aA-zZäöüÄÖÜß\\s+]+:', text)\\\n",
    "            .group().replace(':', '').replace('.', '')\n",
    "            return number\n",
    "        elif re.match('[aA-zZ\\d+\\s+]+:', text):\n",
    "            number = re.match('[aA-zZ\\d+\\s+]+:', text)\\\n",
    "            .group().replace(':', '').replace('.', '')\n",
    "            return number\n",
    "        elif re.match('^M{0,3}(CM|CD|D?C{0,3})(XC|XL|L?X{0,3})(IX|IV|V?I{0,3})(.)$', text):\n",
    "            number = re.match('^M{0,3}(CM|CD|D?C{0,3})(XC|XL|L?X{0,3})(IX|IV|V?I{0,3})(.)$', text)\\\n",
    "            .group().replace(':', '').replace('.', '')\n",
    "            return number\n",
    "        else:\n",
    "            return None\n",
    "    except Exception as e:\n",
    "        print('Extract Number', e)\n",
    "        return None\n",
    "    return None\n",
    "\n",
    "def split_by_multiple_blank_lines(text):\n",
    "    sections = re.split('\\n\\n+', text)\n",
    "    return sections\n",
    "\n",
    "def odt_to_dict(input_file):\n",
    "    txt = odt_to_txt(input_file)\n",
    "    paras = split_by_multiple_blank_lines(txt)\n",
    "    num_para_dict = {extract_numbers(para):para for para in paras if extract_numbers(para)}\n",
    "    if None in num_para_dict:\n",
    "        num_para_dict.pop(None)\n",
    "    return num_para_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5939f997-9583-4b09-87a8-37dd29952a2a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "count_file = 0\n",
    "close_match_sc_dict_gbif_list =  dict()\n",
    "for csv_file_path in all_csv_files_path:\n",
    "    try:\n",
    "        file_id = int(csv_file_path.split(\"/\")[-1].split(\".\")[0])\n",
    "        try:\n",
    "            df = pd.read_csv(filepath_or_buffer=csv_file_path, encoding='utf-8')\n",
    "        except:\n",
    "            df = pd.read_csv(filepath_or_buffer=csv_file_path, encoding='windows_1258', sep='\\t')\n",
    "        df = df.map(remove_extra_space, na_action='ignore')\n",
    "        first_column = df.columns[0]\n",
    "        df = rename_columns(df, first_column)\n",
    "        df['scientificName'] = df['scientificName'].apply(remove_number)\n",
    "        df = df.map(remove_extra_space, na_action='ignore')\n",
    "        df['scientificName'] = df['scientificName'].apply(replace_commas)\n",
    "        df['scientificName'] = df['scientificName'].apply(normalize_to_ascii)\n",
    "        df['scientificName'] = df['scientificName'].apply(remove_roman_numerals)\n",
    "        df = df.map(remove_extra_space, na_action='ignore')\n",
    "        scientificName_list = df['scientificName'].tolist()\n",
    "\n",
    "        # Complete scientificName\n",
    "        df['scientificName'] = [scientificName_list[0]]+ [complete_species_name(scientificName_list, i) for i, j in enumerate(scientificName_list) if i>0]\n",
    "        close_match_sc_dict_gbif_list.update({sc_name:get_close_scname_and_data_gbif_list(sc_name) for sc_name in df['scientificName'].unique().tolist()})\n",
    "\n",
    "        # Fuzzy Match Scientific Names\n",
    "        df[['scientificName_matchingScore_kingdom_CloseGbiflist', 'scientificName_matchingScore_kingdom_CloseGbiflistLength']]\\\n",
    "                    = pd.DataFrame(df['scientificName'].\\\n",
    "                                   apply(get_close_scname_and_data_from_dict_gbif_list, args=(close_match_sc_dict_gbif_list,)).\\\n",
    "                                   tolist(), index=df.index)\n",
    "    \n",
    "        df['scientificName_matchingScore_kingdom_CloseGbif_Candidate'] = [i[0] for i in candidate_selection(df['scientificName_matchingScore_kingdom_CloseGbiflist']\\\n",
    "                                                                                               .tolist())]\n",
    "        df[['scientificNameGbif', 'matchingScoreGbif', 'kingdomGbif']]= pd.DataFrame(df['scientificName_matchingScore_kingdom_CloseGbif_Candidate'].tolist(),index=df.index)\n",
    "\n",
    "\n",
    "        # drop unnamed columns\n",
    "        df.drop(df.columns[df.columns.str.contains('unnamed', case=False)], axis=1, inplace=True)\n",
    "        \n",
    "        # save data\n",
    "        if df['matchingScoreGbif'].mean() >=60:\n",
    "            directory_path = os.path.dirname(csv_file_path)\n",
    "            csv_file_path = csv_file_path.replace('./local_data/data/data_selection/', './data/selected_data/')\n",
    "            # path of cleaned data\n",
    "            csv_file_path_dir = create_directory_from_file_path(csv_file_path)\n",
    "            \n",
    "            # src - dest\n",
    "            num_para_dict = odt_to_dict(os.path.join(directory_path, str(file_id) +'.odt'))\n",
    "            df_odt = pd.DataFrame(list(num_para_dict.items()), columns=['Index', 'Location Description'])\n",
    "            df_odt.to_csv(os.path.join(csv_file_path_dir, str(file_id) +'_odt.csv'), encoding='utf-8', index=False)\n",
    "            df.to_csv(os.path.join(csv_file_path_dir, str(file_id) +'.csv'), encoding='utf-8', index=False)\n",
    "            count_file = count_file + 1 \n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing csv location '{csv_file_path}': {e}\")\n",
    "        print(f\"Error processing csv location '{first_column}': {df.columns}\", end='\\n\\n\\n\\n')\n",
    "\n",
    "print(\"Files with correct scientific names: \", count_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c8f1e94-2480-42a5-876a-88d53842d6a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "## get location Coordinates and Location using Google API\n",
    "\n",
    "import google.generativeai as genai\n",
    "import os\n",
    "import ast\n",
    "from IPython.display import display\n",
    "from key import GEMINI_KEY\n",
    "import json\n",
    "import time\n",
    "\n",
    "\n",
    "genai.configure(api_key=GEMINI_KEY)\n",
    "MODEL_CONFIG = {\n",
    "  \"temperature\": 0.2,\n",
    "  \"top_p\": 0.95,\n",
    "  \"top_k\": 32,\n",
    "  \"max_output_tokens\": 8192,\n",
    "}\n",
    "model = genai.GenerativeModel(model_name = \"gemini-1.0-pro\",\n",
    "                              generation_config = MODEL_CONFIG)\n",
    "\n",
    "def get_coordinates(text):\n",
    "    # Ermitteln Sie die Standortkoordinaten aus dem angegebenen\n",
    "    text = f\"\"\"Find the latitude and longitude of the location described in the given text:{text} \n",
    "               \\n if it is not found then output None\"\"\"\n",
    "    output = \"\"\"\\nHere is the output schema:\\n{\"latitude\":, \"longitude\":}\"\"\"\n",
    "    text = text + output\n",
    "    # print(text)\n",
    "    response = model.generate_content(text, request_options={\"timeout\": 600})\n",
    "    print('Response Coordinates', response.text)\n",
    "    if 'json' in response.text:\n",
    "        try:\n",
    "        # print(response.text)\n",
    "            coordinates = json.loads(response.text.replace(\"```json\", \"\").replace(\"```\", \"\"))\n",
    "            return coordinates['latitude'], coordinates['longitude']\n",
    "        except Exception as e:\n",
    "            print('Error get coordinates: ', e, response.text)\n",
    "    else:\n",
    "        try:\n",
    "            # print(response.text)\n",
    "            coordinates = ast.literal_eval(response.text)\n",
    "            return coordinates['latitude'], coordinates['longitude']\n",
    "        except Exception as e:\n",
    "            print('Error get coordinates: ', e, response.text)\n",
    "    return (None, None)\n",
    "\n",
    "def is_location(text):\n",
    "    # Ermitteln Sie die Standortkoordinaten aus dem angegebenen\n",
    "    text1 = f\"\"\"Is there any location description in the given text:{text}?\n",
    "    Give me answer in Yes and No only without explanation\"\"\"\n",
    "    output = \"\"\"\\nHere is the output schema: {\"isLocationDescription\":} \"\"\"\n",
    "    text = text1 + output\n",
    "    # print(text)\n",
    "    response = model.generate_content(text, request_options={\"timeout\": 600})\n",
    "    print('Response Location', response.text)\n",
    "    if 'json' in response.text:\n",
    "        try:\n",
    "            # print(response.text)\n",
    "            isLocationDescription = json.loads(response.text.replace(\"```json\", \"\").replace(\"```\", \"\"))\n",
    "            #isLocationDescription = ast.literal_eval(response.text)\n",
    "            # print(isLocationDescription)\n",
    "            return isLocationDescription['isLocationDescription']\n",
    "        except Exception as e:\n",
    "            print('Error isLocationDescription: ', e, response.text)\n",
    "    elif '{' in response.text:\n",
    "        try:\n",
    "            isLocationDescription =  json.loads(response.text)\n",
    "            return isLocationDescription['isLocationDescription']\n",
    "        except Exception as e:\n",
    "            print('Error isLocationDescription: ', e, response.text)\n",
    "    else:\n",
    "        try:\n",
    "            return response.text\n",
    "        except Exception as e:\n",
    "            print('Error isLocationDescription: ', e, response.text)\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e63fcc08-6026-47a5-8a16-31d6396983fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "## get location Coordinates and Location using ChatGPT API\n",
    "\n",
    "from key import OPENAI_KEY\n",
    "import json\n",
    "import time\n",
    "from openai import OpenAI\n",
    "import glob\n",
    "import pandas as pd\n",
    "from ast import literal_eval\n",
    "import ast\n",
    "\n",
    "client = OpenAI(api_key=OPENAI_KEY)\n",
    "\n",
    "def get_coordinates(text):\n",
    "    # Ermitteln Sie die Standortkoordinaten aus dem angegebenen\n",
    "    text = f\"\"\"Find the latitude and longitude of the location described in the given text:{text} \n",
    "               \\n and give output in the given schema only without any extra explanation\"\"\"\n",
    "    output = \"\"\"\\nHere is the output schema:\\n{\"latitude\":, \"longitude\":}\"\"\"\n",
    "    text = text + output\n",
    "    # print(text)\n",
    "    response = client.chat.completions.create(model=\"gpt-4o\",\n",
    "                                              messages=[{\"role\": \"user\", \"content\":text}],\n",
    "                                              temperature=0.2,max_tokens=256,\n",
    "                                              top_p=0.95,\n",
    "                                              frequency_penalty=0,\n",
    "                                              presence_penalty=0)\n",
    "    # response = model.generate_content(text, request_options={\"timeout\": 600})\n",
    "    print('Response Coordinates', response.choices[0].message.content)\n",
    "    if 'json' in response.choices[0].message.content:\n",
    "        try:\n",
    "        # print(response.text)\n",
    "            coordinates = json.loads(response.choices[0].message.content.replace(\"```json\", \"\").replace(\"```\", \"\"))\n",
    "            return coordinates['latitude'], coordinates['longitude']\n",
    "        except Exception as e:\n",
    "            print('Error get coordinates: ', e, response.choices[0].message.content)\n",
    "    else:\n",
    "        try:\n",
    "            # print(response.text)\n",
    "            coordinates = ast.literal_eval(response.choices[0].message.content)\n",
    "            return coordinates['latitude'], coordinates['longitude']\n",
    "        except Exception as e:\n",
    "            print('Error get coordinates: ', e, response.choices[0].message.content)\n",
    "    return (None, None)\n",
    "\n",
    "def is_location(text):\n",
    "    # Ermitteln Sie die Standortkoordinaten aus dem angegebenen\n",
    "    text1 = f\"\"\"Is there any location description in the given text:{text}?\n",
    "    Give me answer in Yes and No only without explanation\"\"\"\n",
    "    output = \"\"\"\\nHere is the output schema: {\"isLocationDescription\":\"\"} \"\"\"\n",
    "    text = text1 + output\n",
    "    response = client.chat.completions.create(model=\"gpt-4o\",\n",
    "                                              messages=[{\"role\": \"user\", \"content\":text}],\n",
    "                                              temperature=0.2,max_tokens=256,\n",
    "                                              top_p=0.95,\n",
    "                                              frequency_penalty=0,\n",
    "                                              presence_penalty=0)\n",
    "    print('Response Location', response.choices[0].message.content)\n",
    "    if 'json' in response.choices[0].message.content:\n",
    "        try:\n",
    "            # print(response.text)\n",
    "            isLocationDescription = json.loads(response.choices[0].message.content.replace(\"```json\", \"\").replace(\"```\", \"\"))\n",
    "            #isLocationDescription = ast.literal_eval(response.text)\n",
    "            # print(isLocationDescription)\n",
    "            return isLocationDescription['isLocationDescription']\n",
    "        except Exception as e:\n",
    "            print('Error isLocationDescription: ', e, response.choices[0].message.content)\n",
    "    elif '{' in response.choices[0].message.content:\n",
    "        try:\n",
    "            isLocationDescription =  json.loads(response.choices[0].message.content)\n",
    "            return isLocationDescription['isLocationDescription']\n",
    "        except Exception as e:\n",
    "            print('Error isLocationDescription: ', e, response.choices[0].message.content)\n",
    "    else:\n",
    "        try:\n",
    "            return response.choices[0].message.content\n",
    "        except Exception as e:\n",
    "            print('Error isLocationDescription: ', e, response.choices[0].message.content)\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68d1f702-defe-472e-b372-aedc28464fde",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "directory_path  = \"./data/selected_data/**/\"  # Replace with the path to your folder\n",
    "all_csv_odt_files_path  = glob.glob(f\"{directory_path}/*_odt.csv\", recursive=True)\n",
    "\n",
    "for csv_odt_file_path in all_csv_odt_files_path:\n",
    "    try:\n",
    "        df = pd.read_csv(filepath_or_buffer=csv_odt_file_path, encoding='utf-8')\n",
    "        df['isLocationDescription'] = df['Location Description'].apply(is_location)\n",
    "        df['coordinates'] = df['Location Description'].apply(get_coordinates)\n",
    "        df[['latitude', 'longitude']] = pd.DataFrame(df['coordinates'].tolist(), index=df.index)\n",
    "        df.to_csv(csv_odt_file_path, encoding='utf-8', index=False)\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing location '{csv_odt_file_path}': {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18c2eed3-ced8-48ab-8b34-c84b21658162",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter files not having coordinate or location data\n",
    "directory_path  = \"./data/selected_data/**/\"  # Replace with the path to your folder\n",
    "all_csv_files_path  = glob.glob(f\"{directory_path}/*.csv\", recursive=True)\n",
    "all_odt_files_path = glob.glob(f\"{directory_path}/*_odt.csv\", recursive=True)\n",
    "\n",
    "all_csv_files_path = list(set(all_csv_files_path)-set(all_odt_files_path))\n",
    "print(len(all_csv_files_path), len(all_odt_files_path))\n",
    "print(all_csv_files_path[0:10],'\\n\\n' ,all_odt_files_path[0:10],'\\n\\n')\n",
    "\n",
    "# if coordinate not yes delete csv and odt file\n",
    "for odt_file in all_odt_files_path:\n",
    "    df = pd.read_csv(odt_file)\n",
    "    try:\n",
    "        if (df['isLocationDescription']==\"Yes\").sum()/len(df['isLocationDescription'])>=0.5:\n",
    "                pass\n",
    "        else:\n",
    "            # delete data files\n",
    "            os.remove(odt_file.replace('_odt', ''))\n",
    "            os.remove(odt_file)\n",
    "    except Exception as e:\n",
    "        print(f\"Error deleting location '{odt_file}': {e}\")\n",
    "        try:\n",
    "            os.remove(odt_file.replace('_odt', ''))\n",
    "            os.remove(odt_file)\n",
    "        except Exception as e::\n",
    "            print(f\"Error final deleting location '{odt_file}': {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7473cf39-a020-4927-8037-a7848826d6f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# final cleaning of files\n",
    "import pandas as pd\n",
    "from multiprocessing import Pool, cpu_count\n",
    "import glob\n",
    "from ast import literal_eval\n",
    "import re\n",
    "\n",
    "def melt_df(df):\n",
    "    ## columns rotation rules\n",
    "     return df.melt(id_vars=['scientificName', \n",
    "                             'scientificName_matchingScore_kingdom_CloseGbiflist',\n",
    "                             'scientificName_matchingScore_kingdom_CloseGbiflistLength',\n",
    "                             'scientificName_matchingScore_kingdom_CloseGbif_Candidate',\n",
    "                             'scientificNameGbif', 'matchingScoreGbif', 'kingdomGbif'], \\\n",
    "                    var_name=\"location_index\", \\\n",
    "                    value_name=\"organismQuantity\") # or treat as locality\n",
    "\n",
    "def add_location_coordinates(location_index, location_dict, csv_file_path):\n",
    "    try:\n",
    "        loc_desc_coor = [loc_desc_coor for index, loc_desc_coor in location_dict.items() \\\n",
    "                 if re.sub('[^A-Za-z0-9]+', '', str(location_index).lower().strip()) \\\n",
    "                    in str(index).lower().strip()][0]\n",
    "        # print(loc_desc_coor)\n",
    "        return loc_desc_coor[0], literal_eval(loc_desc_coor[1])\n",
    "    except Exception as e:\n",
    "        return None, (None, None)\n",
    "        # print(f\"Error location '{location_index, csv_file_path}': {e}\")\n",
    "    return None, (None, None)\n",
    "\n",
    "\n",
    "def enriched_df(df, image_or_file_id):\n",
    "    df_meta = pd.read_csv('./local_data/imageId_metapath_metadata.csv', encoding='Utf')\n",
    "    df_meta = df_meta[df_meta['Image_Id']==image_or_file_id]\n",
    "    # print (df_meta.head())\n",
    "    df['eventDate'] = int(df_meta['eventDate'].values[0])\n",
    "    df['year'] = int(df_meta['year'].values[0])\n",
    "    df['publicationTitle']= df_meta['publicationTitle'].values[0]\n",
    "    df['publicationYear'] =  int(df_meta['publicationYear'].values[0] )\n",
    "    df['collectionCode'] = df_meta['collectionCode'].values[0]\n",
    "    df['catalogNumber'] = int(df_meta['catalogNumber'].values[0])\n",
    "    df['publicationAuthors'] = df_meta['publicationAuthors'].values[0]\n",
    "    df['authorityURI'] = df_meta['authorityURI'].values[0]\n",
    "    df['authorityValue'] = int(df_meta['authorityValue'].values[0])\n",
    "    \n",
    "    # ToDo --- Fixed Scale\n",
    "    df['organismQuantityType'] = 'Braun-Blanquet Scale'\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "604fdcff-07cf-4109-bacc-7cd538815a5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of cpu :  12\n",
      "Error cleaning data './data/selected_data/CSV_05_Dec/329.csv': Columns must be same length as key\n"
     ]
    }
   ],
   "source": [
    "def cleaning_data(csv_file_path):\n",
    "    # getfile and melt it according to columns\n",
    "    try:\n",
    "        file_id = int(csv_file_path.split(\"/\")[-1].split(\".\")[0])\n",
    "        \n",
    "        df = pd.read_csv(filepath_or_buffer=csv_file_path, encoding='utf-8')\n",
    "        df = melt_df(df)\n",
    "    \n",
    "        # Creating new columns and feeding data\n",
    "        df['basisOfRecord']='Human Observation'\n",
    "    \n",
    "        df_location = pd.read_csv(csv_file_path.replace('.csv', '_odt.csv'), encoding='utf-8')\n",
    "        location_dict = dict(zip(df_location['Index'], \n",
    "                        tuple(zip(df_location['Location Description'], df_location['coordinates']))))\n",
    "        ## add location name\n",
    "        df[['locality', 'coordinates']] = pd.DataFrame(df['location_index'].\\\n",
    "                                                   apply(add_location_coordinates, args=(location_dict, csv_file_path)).tolist(),\n",
    "                                                   index=df.index)\n",
    "    \n",
    "        # print(df['location_index'].apply(add_location_coordinates, args=(location_dict, csv_file_path)).tolist())\n",
    "        ## add coordinates\n",
    "        # df['coordinates'] = df['location_index'].apply(add_coordinates)\n",
    "        df[['latitude', 'longitude']] = pd.DataFrame(df['coordinates'].tolist(), index=df.index)\n",
    "    \n",
    "        ## ToDo -  add data from meta data  \n",
    "        # df = enriched_df(df, file_id)\n",
    "        # directory_path = os.path.dirname(csv_file_path)\n",
    "        csv_file_path = csv_file_path.replace('./data/selected_data/', './data/cleaned_data/')\n",
    "        csv_file_path_dir = create_directory_from_file_path(csv_file_path)\n",
    "        # print(directory_path, csv_file_path, csv_file_path_dir, file_id)\n",
    "        df.to_csv(csv_file_path, encoding='utf-8', index=False)\n",
    "    except Exception as e:\n",
    "        print(f\"Error cleaning data '{csv_file_path}': {e}\") \n",
    "    \n",
    "\n",
    "print(\"Number of cpu : \", cpu_count())\n",
    "p = Pool(4)\n",
    "# Todo - only two csv files\n",
    "directory_path  = \"./data/selected_data/**/\"  # Replace with the path to your folder\n",
    "all_csv_odt_files_path  = glob.glob(f\"{directory_path}/*.csv\", recursive=True)\n",
    "all_odt_files_path = glob.glob(f\"{directory_path}/*_odt.csv\", recursive=True)\n",
    "all_csv_files_path = list(set(all_csv_odt_files_path)-set(all_odt_files_path))\n",
    "\n",
    "# p.map(cleaning_data, zip(all_csv_files_path, all_odt_files_path))\n",
    "# print(all_csv_files_path)\n",
    "for csv_file_path in all_csv_files_path:\n",
    "    cleaning_data(csv_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "353b404d-637e-47a7-a5d9-894e9ce5003c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf4b0ebe-e727-45c2-840e-1b2a11ffe7cd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
