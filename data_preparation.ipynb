{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "803ac6fe-2d50-4ef9-8164-1c0d42be2688",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from geopy.geocoders import Nominatim\n",
    "from openai import OpenAI\n",
    "import requests\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "989310cf-706e-4b64-bf59-80843b92ab0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## coordinates\n",
    "coordinates_dict = {\n",
    "    'Gebirgsbäche im Sauerland': {'latitude': 51.1234, 'longitude': 8.5678},\n",
    "    'Plankton der Werse bei Münster': {'latitude': 51.9876, 'longitude': 7.6543},\n",
    "    'Ruhr': {'latitude': 51.4567, 'longitude': 7.8901},\n",
    "    'Lippe': {'latitude': 51.2345, 'longitude': 8.9012},\n",
    "    'Eder': {'latitude': 50.8765, 'longitude': 8.3456},\n",
    "    'Salinen und Salzgräben im südlichen Gebiet': {'latitude': 50.5432, 'longitude': 8.7654},\n",
    "    'desgl. im nördlichen Gebiet': {'latitude': 50.7890, 'longitude': 8.1234},\n",
    "    'Plankton der Talsperren': {'latitude': 51.4321, 'longitude': 7.5432},\n",
    "    'Plankton des Dortmund-Ems Kanals': {'latitude': 51.8765, 'longitude': 7.2109},\n",
    "    'Teiche und Moor stellen ,“Kipshagen ”': {'latitude': 51.6543, 'longitude': 8.0987},\n",
    "    'Seen, Weiher und Moorstellen ,“Heiliges Meer\"': {'latitude': 52.3456, 'longitude': 7.8901},\n",
    "    'Moore im Sauer und Münsterland': {'latitude': 51.2345, 'longitude': 7.5432},}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2de62b70-4c0c-4494-8246-2989edbca54c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('./local_data/data/609.csv', encoding='utf-8')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a767859d-9fcd-4600-a310-2a112d67fda4",
   "metadata": {},
   "outputs": [],
   "source": [
    "## replacement values rules\n",
    "# df = df.replace(to_replace=['+', '‒'], value=['present', 'absent'])\n",
    "# df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbb252cd-e3c3-402d-ae0e-3dfd18cf939b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## columns rotation rules\n",
    "df = df.melt(id_vars=[\"Unnamed: 0\"], \n",
    "        var_name=\"location\", \n",
    "        value_name=\"organismQuantity\") # or treat as locality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2177285-3f0c-4811-bcbf-b9aaf666b082",
   "metadata": {},
   "outputs": [],
   "source": [
    "# renaming columns\n",
    "df.rename(columns={'Unnamed: 0':'scientificName'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb3b750e-6e2d-4b1c-8ca7-d0aeae572879",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating new columns and feeding data\n",
    "df['basisOfRecord']='Human Observation'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e90fc48e-7ab5-44f9-b283-c1bce09eab82",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "558825f3-ca27-43fc-8e44-fa8d6e2dc93d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b786340-a6f8-4aef-936b-445a7e2662ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Get the correct names of scientific species\n",
    "\n",
    "# Clean and correct names\n",
    "def remove_number(text):\n",
    "    if text !=text:\n",
    "        return text\n",
    "    text = re.sub('^[0-9.\\*]*', '', text, count=1)\n",
    "    text = re.sub('^[aA-zZ]\\\\)', '', text, count=1)\n",
    "    return text\n",
    "\n",
    "def replace_commas(text):\n",
    "    if text !=text:\n",
    "        return text\n",
    "    text = re.sub('^, ,', ' ,, ', text)\n",
    "    text = re.sub('^，，', ',,', text)\n",
    "    text = re.sub('^,,', ' ,, ', text)\n",
    "    text = re.sub('^,', ' ,, ', text)\n",
    "    text = re.sub('^, , , ,', ' ,, ,, ', text)\n",
    "    return text\n",
    "\n",
    "def remove_extra_space(text):\n",
    "    if text !=text:\n",
    "        return text\n",
    "    text = re.sub(' +', ' ', text).strip()\n",
    "    return text\n",
    "\n",
    "def complete_species_name(scientificName_list, i):\n",
    "    prev = scientificName_list[i-1].split()[0]\n",
    "    # print('0', scientificName_list[i], '\\t', scientificName_list[i-1])\n",
    "    # print(prev)\n",
    "    scientificName_list[i] = scientificName_list[i].replace(',,', prev)\n",
    "    # print('1', scientificName_list[i], '\\t', scientificName_list[i-1], end='\\n\\n')\n",
    "    return scientificName_list[i]\n",
    "\n",
    "def get_kingdom(text):\n",
    "    url = \"https://api.gbif.org/v1/species/search?q={}&origin=SOURCE&status=ACCEPTED&strict=true\".format(text)\n",
    "    payload = {}\n",
    "    headers = {'Authorization': 'Basic YWtodnlhczA6VnlAJDEyMzQ='}\n",
    "    response = requests.request(\"GET\", url, headers=headers, data=payload)\n",
    "    try:\n",
    "        if response.status_code==200:\n",
    "            return response.json()['results'][0]['kingdom']\n",
    "        else:\n",
    "            return None\n",
    "    except:\n",
    "        return None\n",
    "    \n",
    "def correct_species_name(text):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57559c8d-1067-4d6d-acea-2ef1c4bf174b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.map(remove_extra_space, na_action='ignore')\n",
    "df['scientificName'] = df['scientificName'].apply(remove_number)\n",
    "df = df.map(remove_extra_space, na_action='ignore')\n",
    "df['scientificName'] = df['scientificName'].apply(replace_commas)\n",
    "df = df.map(remove_extra_space, na_action='ignore')\n",
    "df.to_csv('./data/609_cleaned.csv', encoding='utf-8', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "870985ec-5045-469a-9958-911b62112aa6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# this API is very Slow\n",
    "scientificName_list = df['scientificName'].tolist()\n",
    "df['scientificName'] = [scientificName_list[0]]+ [complete_species_name(scientificName_list, i) for i, j in enumerate(scientificName_list) if i>0]\n",
    "df.to_csv('./data/609_cleaned.csv', encoding='utf-8', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df32746a-692f-4270-be35-67eb8fe6df6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.sample(n=50, random_state=1)\n",
    "df.to_csv('./data/609_cleaned_50_records_random.csv', encoding='utf-8', index=False)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7717f78-6b82-44d6-a87e-eda9a882c8fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get and add Kingdom\n",
    "df['kingdom'] = df['scientificName'].apply(get_kingdom)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28a384c5-dae7-45ec-b563-82209ab635ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get coordinates\n",
    "geolocator = Nominatim(user_agent=\"your_app_name\")  # Replace 'your_app_name' with a unique name for your application\n",
    "\n",
    "# Function to get coordinates for a location\n",
    "def get_coordinates(location):\n",
    "    try:\n",
    "        # Use geopy to get location coordinates\n",
    "        location_data = geolocator.geocode(location, language='de')\n",
    "        if location_data:\n",
    "            return location_data.latitude, location_data.longitude\n",
    "        else:\n",
    "            return None, None\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing location '{location}': {e}\")\n",
    "        return None, None\n",
    "\n",
    "# Apply the function to the \"location\" column and create new \"latitude\" and \"longitude\" columns\n",
    "print(df.shape)\n",
    "df['coordinates'] = df['location'].apply(get_coordinates)\n",
    "print(df.shape)\n",
    "df[['latitude', 'longitude']] = pd.DataFrame(df['coordinates'].tolist(), index=df.index)\n",
    "\n",
    "# Save the updated DataFrame to a new CSV file\n",
    "# df.to_csv('./data/609_10_records_location_coordinate.csv', encoding='utf-8', index=False)\n",
    "\n",
    "# Display the DataFrame with coordinates\n",
    "print(df[['location', 'latitude', 'longitude']])\n",
    "df[['location', 'latitude', 'longitude']].drop_duplicates()\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8f5a1b1-5197-4bb9-990b-dcc5f1aaf63d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## get coordinates\n",
    "\n",
    "# Replace 'your_api_key' with your actual OpenCage Geocoding API key\n",
    "api_key = ''\n",
    "base_url = 'https://api.opencagedata.com/geocode/v1/json'\n",
    "\n",
    "\n",
    "# Function to get coordinates for a location using the OpenCage Geocoding API\n",
    "def get_coordinates(location):\n",
    "    try:\n",
    "        params = {\n",
    "            'q': location,\n",
    "            'key': api_key,\n",
    "            'language': 'en',  # Specify language for results\n",
    "        }\n",
    "        response = requests.get(base_url, params=params)\n",
    "        data = response.json()\n",
    "\n",
    "        if data['results']:\n",
    "            latitude = data['results'][0]['geometry']['lat']\n",
    "            longitude = data['results'][0]['geometry']['lng']\n",
    "            return latitude, longitude\n",
    "        else:\n",
    "            return None, None\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing location '{location}': {e}\")\n",
    "        return None, None\n",
    "\n",
    "# Apply the function to the \"location\" column and create new \"latitude\" and \"longitude\" columns\n",
    "df['coordinates'] = df['location'].apply(get_coordinates)\n",
    "df[['latitude', 'longitude']] = pd.DataFrame(df['coordinates'].tolist(), index=df.index)\n",
    "\n",
    "# Save the updated DataFrame to a new CSV file\n",
    "output_file_path = 'output_file.csv'  # Replace 'output_file.csv' with your desired output file path\n",
    "df.to_csv(output_file_path, index=False)\n",
    "\n",
    "# Display the DataFrame with coordinates\n",
    "print(df[['location', 'latitude', 'longitude']])\n",
    "df[['location', 'latitude', 'longitude']].drop_duplicates()\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bc075dc-64d5-4016-87f0-590e36451b7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Get coordinates using prefilled dict\n",
    "\n",
    "def get_coordinates(location):\n",
    "    if location in coordinates_dict:\n",
    "        return coordinates_dict[location]['latitude'], coordinates_dict[location]['longitude']\n",
    "    return None, None\n",
    "    \n",
    "df['coordinates'] = df['location'].apply(get_coordinates)\n",
    "df[['latitude', 'longitude']] = pd.DataFrame(df['coordinates'].tolist(), index=df.index)\n",
    "\n",
    "df.to_csv('./data/609_cleaned_coordinates.csv', encoding='utf-8', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eacea386-7845-401c-b9c0-fb0133d498c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "## get coordinates - Translation code\n",
    "\n",
    "import key\n",
    "\n",
    "import importlib\n",
    "importlib.reload(key)\n",
    "\n",
    "\n",
    "\n",
    "client = OpenAI(api_key=key.key)\n",
    "\n",
    "\n",
    "def translate_text_with_chatgpt(text, language=\"en\"):\n",
    "    # Define the prompt for translation\n",
    "    prompt = f\"Translate the following German text to {language}: {text}\"\n",
    "\n",
    "    # Make an API call to ChatGPT for translation\n",
    "    stream = client.chat.completions.create( model=\"gpt-3.5-turbo-1106\", \n",
    "                                            messages=[{\"role\": \"user\", \"content\": \"Say this is a test\"}],\n",
    "                                            stream=True,)\n",
    "\n",
    "    for chunk in stream:\n",
    "        print(chunk.choices[0].delta.content or \"\", end=\"\")\n",
    "    \n",
    "    \"\"\"response = openai.Completion.create(\n",
    "        engine=\"text-davinci-002\",  # You can experiment with different engines\n",
    "        prompt=prompt,\n",
    "        max_tokens=150  # Adjust as needed\n",
    "    )\"\"\"\n",
    "\n",
    "    # Extract the translated text from the response\n",
    "    # translated_text = response.choices[0].text.strip()\n",
    "\n",
    "    return None\n",
    "\n",
    "# Translate the text in the specified column using ChatGPT\n",
    "df['scientificName_en'] = df['scientificName'].apply(lambda x: translate_text_with_chatgpt(x))\n",
    "\n",
    "# Save the translated DataFrame to a new CSV file\n",
    "# df.to_csv(output_csv, index=False)\n",
    "df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
